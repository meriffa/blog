# .NET Core Troubleshooting (Managed Memory Leak)

This article outlines the steps to troubleshoot Managed Memory Leak issue in a .NET Core application.

## Problem

Managed Memory Leak is an application issue where the managed memory usage grows over time. The goal of troubleshooting such issue is to identify the source of the memory growth and refactor the code to resolve the problem.

Let's review the following sample code:

```
private static readonly Workforce<Employee> workforce = new() { Employees = [] };

private async Task StartMemoryConsumptionTask(int allocationBatch, int textLength, int allocationDelay, CancellationToken cancellationToken)
{
    while (true)
    {
        for (int i = 0; i < allocationBatch; i++)
            workforce.Employees.Add(GetEmployee(textLength));
        displayService.WriteInformation($"Employee instances allocated (Count = {allocationBatch}, Total = {workforce.Employees.Count}).");
        if (cancellationToken.IsCancellationRequested)
            break;
        await Task.Delay(allocationDelay, cancellationToken);
    }
}
```

In this code snippet, the function `StartMemoryConsumptionTask()` allocates batches of `Employee` instances created by the `GetEmployee()` function. The `Employee` instances are added to `workforce.Employees`, which is a property of type `List<Employee>`. The `while(true)` loop will continue to allocate managed instances until cancellation is signaled via the `cancellationToken` parameter or the available memory is exhausted and OOM exception is generated.

## Prerequisites

Before we investigate the Managed Memory Leak, we can confirm the issue by running the application and starting a monitoring session using `dotnet-counters`:

```
dotnet-counters monitor -p <PID> --refresh-interval 1 --counters System.Runtime[dotnet.gc.heap.total_allocated]
```

Output:
```
Name                                        Current Value
[System.Runtime]                                                                                                                                                                    
    dotnet.gc.heap.total_allocated (By)     14,293,488

    dotnet.gc.heap.total_allocated (By)     19,113,328

    dotnet.gc.heap.total_allocated (By)     24,329,832
```

If we leave the application running, we will see that the managed GC heap allocation grows steadily.

To investigate the issue, we need to take two or more core dumps using `createdump` and load them into [LLDB](https://lldb.llvm.org/). For more details, see [Create .NET Core Dumps On Linux](/Resources/Articles/Prerequisites/Create%20.NET%20Core%20Dumps%20On%20Linux.md) and [Load .NET Core Dumps In LLDB On Linux](/Resources/Articles/Prerequisites/Load%20.NET%20Core%20Dumps%20In%20LLDB%20On%20Linux.md).

## Analysis

We start the Managed Memory Leak analysis by comparing the managed heaps between the first and second core dumps:

First Core Dump:
```
dumpheap -stat -min 1000
```

Output:
```
Statistics:
          MT Count  TotalSize Class Name
7fa04a7bce80     1      1,600 System.Collections.Concurrent.ConcurrentDictionary<Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceIdentifier, System.Object>+VolatileNode[]
7fa04a7bcae8     1      1,600 System.Collections.Concurrent.ConcurrentDictionary<Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceCacheKey, Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceCallSite>+VolatileNode[]
7fa04a91da68     1      1,912 System.DateTime[]
7fa04a29c718     2      2,096 System.Reflection.RuntimeMethodInfo[]
7fa04a5ab5b0     1      2,160 System.Collections.Generic.Dictionary<System.Int32, System.IO.FileSystemWatcher+RunningInstance+WatchedDirectory>+Entry[]
7fa049eb90c8     1      2,424 System.String[]
7fa04b2425a8     2      3,888 System.Diagnostics.Tracing.EventSource+EventMetadata[]
7fa04b23eb08     1      4,024 System.Threading.PortableThreadPool+HillClimbing+LogEntry[]
7fa04a917738     2      4,080 System.Collections.Generic.Dictionary<System.String, System.ConsoleKeyInfo>+Entry[]
7fa04a91fb18     3      5,040 System.TimeZoneInfo+AdjustmentRule[]
7fa04a7be118     2      5,088 System.Collections.Generic.Dictionary<Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceIdentifier, Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteFactory+ServiceDescriptorCacheItem>+Entry[]
7fa04aeba0f8     3      7,240 ByteZoo.Blog.Common.Models.Business.Employee[]
7fa04a3291a0     4      8,640 System.Collections.Hashtable+Bucket[]
7fa04a324130     4      8,640 System.Collections.Generic.Dictionary<System.String, System.String>+Entry[]
7fa049da2278     2     24,528 System.Object[]
7fa04a2ce908     9     44,185 System.Byte[]
5605f6eb4820    12     45,280 Free
7fa049df8d38     4     46,272 System.Int32[]
7fa049ea7d30    23    114,702 System.Char[]
7fa049dfbe30 1,243 12,230,346 System.String
Total 1,321 objects, 12,563,745 bytes
```

Second Core Dump:
```
dumpheap -stat -min 1000
```

Output:
```
Statistics:
          MT Count  TotalSize Class Name
7fa04a7bce80     1      1,600 System.Collections.Concurrent.ConcurrentDictionary<Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceIdentifier, System.Object>+VolatileNode[]
7fa04a7bcae8     1      1,600 System.Collections.Concurrent.ConcurrentDictionary<Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceCacheKey, Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceCallSite>+VolatileNode[]
7fa04b2425a8     1      1,848 System.Diagnostics.Tracing.EventSource+EventMetadata[]
7fa04a91fb18     1      1,920 System.TimeZoneInfo+AdjustmentRule[]
7fa04a5ab5b0     1      2,160 System.Collections.Generic.Dictionary<System.Int32, System.IO.FileSystemWatcher+RunningInstance+WatchedDirectory>+Entry[]
7fa049eb90c8     1      2,424 System.String[]
7fa04a917738     1      2,872 System.Collections.Generic.Dictionary<System.String, System.ConsoleKeyInfo>+Entry[]
7fa04a7be118     1      3,584 System.Collections.Generic.Dictionary<Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceIdentifier, Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteFactory+ServiceDescriptorCacheItem>+Entry[]
7fa04b23eb08     1      4,024 System.Threading.PortableThreadPool+HillClimbing+LogEntry[]
7fa04a324130     3      6,480 System.Collections.Generic.Dictionary<System.String, System.String>+Entry[]
7fa04aeba0f8     1      8,216 ByteZoo.Blog.Common.Models.Business.Employee[]
7fa049da2278     2     24,528 System.Object[]
7fa049df8d38     1     24,624 System.Int32[]
7fa04a2ce908     5     34,577 System.Byte[]
7fa049ea7d30     6     43,154 System.Char[]
5605f6eb4820   130  1,795,736 Free
7fa049dfbe30 1,826 18,163,032 System.String
Total 1,983 objects, 20,122,379 bytes
```

The `dumpheap` output displays objects sorted by overall memory usage (`TotalSize`) by type. We start the comparison analysis from the bottom up. The output clearly shows that `System.String` instances have grown from 1,243 (12,230,346 bytes) to 1,826 (18,163,032 bytes). This is consistent with the growth that we have seen from the `dotnet-counters` output earlier.

> [!NOTE]
> The output also shows significant growth is the number of `Free` blocks - from 12 (45,280 bytes) to 130 (1,795,736 bytes). These show memory fragmentation, which is a different issue and is outside the scope of this article.

A better approach is to use [ClrMD](https://github.com/microsoft/clrmd) and automate the comparison of managed heaps between the first and second core dumps. The article's source code contains such a comparison tool, which we can use as follows:

```
dotnet ByteZoo.Blog.App.dll ClrMD-HeapCompare --dumpFile CoreDump_Full.<PID>.1 --compareFile CoreDump_Full.<PID>.2 --totalSizeIncrease 10 --totalSizeMinimum 1000000 --excludeFree
```

Output:
```
ByteZoo.Blog application started.
Type: MT = 0x00007FA049DFBE30, Count [6397, 4789], Total Size = [12499230, 18303822], Name = System.String
ByteZoo.Blog application completed.
```

> [!NOTE]
> The tool compares both managed heaps and displays all objects where the total size change is greater than 10% and the total size is at least 1,000,000 bytes excluding the free blocks.

The output clearly shows that the managed memory growth is a result of `System.String` instance growth - total size increases from 15 MB to 23 MB. The object count in this case decreases from 6,397 to 4,789 due to GC happening between the two core dumps. We can confirm the GC instance using:

First Core Dump:
```
dumpgcdata
```

Output:
```
concurrent GCs                : 0
compacting GCs                : 0
promoting GCs                 : 0
GCs that did demotion         : 0
card bundles                  : 0
elevation logic               : 0
...
```

Second Core Dump:
```
dumpgcdata
```

Output:
```
concurrent GCs                : 0
compacting GCs                : 0
promoting GCs                 : 1
GCs that did demotion         : 0
card bundles                  : 1
elevation logic               : 0
...
```

Next, we need to find which GC root chains the `System.String` instances belong to. We use the following against the second core dump:

```
dumpheap -mt 0x00007FA049DFBE30 -min 1000
```

Output:
```
...
    7f6039dcda00     7fa049dfbe30         10,022 
    7f6039dd0160     7fa049dfbe30         10,022 
    7f6039dd2910     7fa049dfbe30         10,022 
    7f6039dd5050     7fa049dfbe30         10,022 
    7f6039dd77b0     7fa049dfbe30         10,022 
    7f6039dd9f60     7fa049dfbe30         10,022 
    7f6039ddc6a0     7fa049dfbe30         10,022 
    7f6039de02e8     7fa049dfbe30          2,914 
    7f6039de1068     7fa049dfbe30          2,914 
    7f6039de9068     7fa049dfbe30          2,914 
    7f6039def0c8     7fa049dfbe30          2,914 
    7f6039df1358     7fa049dfbe30          2,914 
    7f6039df3008     7fa049dfbe30          2,914 
    7f6039df9648     7fa049dfbe30          2,914 
    7f6039dfb088     7fa049dfbe30          2,914 
    7f6039e01870     7fa049dfbe30          2,914 
    7f6039e03008     7fa049dfbe30          2,914 
    7f6039e09bb8     7fa049dfbe30          2,914 
    7f6039e0b088     7fa049dfbe30          2,914 
    7f6039e11e28     7fa049dfbe30          2,914 
    7f6039e13008     7fa049dfbe30          2,914 
    7f6039e1a130     7fa049dfbe30          2,914 
    7f6039e1b088     7fa049dfbe30          2,914 
    7f6039e22340     7fa049dfbe30          2,914 
    7f6039e23008     7fa049dfbe30          2,914 
    7f6039e2b088     7fa049dfbe30          2,914 

Statistics:
          MT Count  TotalSize Class Name
7fa049dfbe30 1,826 18,163,032 System.String
Total 1,826 objects, 18,163,032 bytes
```

We start examining the GC roots of the latest `System.String` instances (bottom of the `dumpheap` list, the highest address value):

```
gcroot 7f6039e2b088
```

Output:
```
Found 0 unique roots.
```

We repeat the `gcroot` command, working our way up through the `dumpheap` list until we start seeing GC root chains:

```
gcroot -nostacks 7f6039ddc6a0
```

Output:
```
HandleTable:
    00007fa0c86013e8 (strong handle)
          -> 7f6034000028     System.Object[] 
          -> 7f60368c2e98     ByteZoo.Blog.Common.Models.Business.Workforce<ByteZoo.Blog.Common.Models.Business.Employee> (static variable: System.Void.handler)
          -> 7f60368c2eb0     System.Collections.Generic.List<ByteZoo.Blog.Common.Models.Business.Employee> 
          -> 7f6039952a20     ByteZoo.Blog.Common.Models.Business.Employee[] 
          -> 7f6039dd9ed8     ByteZoo.Blog.Common.Models.Business.Employee 
          -> 7f6039dd9f28     ByteZoo.Blog.Common.Models.People.PersonName 
          -> 7f6039ddc6a0     System.String 

Found 1 unique roots.
```

The output shows that the `System.String` instance is kept alive by a `Workforce<Employee>` static variable at address `0x7F60368C2E98`. If we continue to check the `System.String` roots, we will see that they follow the same pattern by being part of the `Workforce<Employee>` GC root chain.

> [!NOTE]
> The last `gcroot` command specifies `-nostacks` option to exclude any stack roots. This is done only to simplify the output in this article. In most cases, you need to check for all types of GC roots and not specify this option.

## Conclusion

The analysis shows that there is a Managed Memory Leak caused by unlimited growth of `System.String` instances that are kept alive by a `Workforce<Employee>` static variable.

To solve this issue, we need to review the code and identify the sources of instance allocations in the `Workforce<Employee>` GC root chain starting from the top of the chain - `List<Employee>`, `Employee[]`, `Employee`, `PersonName` and `System.String`. In this scenario the allocation happens on line `workforce.Employees.Add(GetEmployee(textLength))` where a new `Employee` instance is added to the `List<Employee>` property of `Workforce<Employee>`.

## References

* [.NET Core Concepts (Summary)](/Resources/Articles/Concepts/.NET%20Core%20Concepts%20(Summary).md)
* [.NET Core Troubleshooting (Hub)](/Resources/Articles/Troubleshooting/.NET%20Core%20Troubleshooting%20(Hub).md)
* [Article Source Code](/Sources)

<!--- Category = Tags = .NET Core, .NET Troubleshooting, .NET Analysis, Linux --->